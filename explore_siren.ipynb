{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5OgDOJ-vBua"
      },
      "source": [
        "# Siren Exploration\n",
        "\n",
        "This is a colab to explore properties of the Siren MLP, proposed in our work [Implicit Neural Activations with Periodic Activation Functions](https://vsitzmann.github.io/siren).\n",
        "\n",
        "\n",
        "We will first implement a streamlined version of Siren for fast experimentation. This lacks the code to easily do baseline comparisons - please refer to the main code for that - but will greatly simplify the code!\n",
        "\n",
        "**Make sure that you have enabled the GPU under Edit -> Notebook Settings!**\n",
        "\n",
        "We will then reproduce the following results from the paper: \n",
        "* [Fitting an image](#section_1)\n",
        "* [Fitting an audio signal](#section_2)\n",
        "* [Solving Poisson's equation](#section_3)\n",
        "* [Initialization scheme & distribution of activations](#activations)\n",
        "* [Distribution of activations is shift-invariant](#shift_invariance)\n",
        "\n",
        "We will also explore Siren's [behavior outside of the training range](#out_of_range).\n",
        "\n",
        "Let's go! First, some imports, and a function to quickly generate coordinate grids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "11vf3suZvBui"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import numpy as np\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cpmzAkFP9YNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbAgWmwOvBuk"
      },
      "source": [
        "Now, we code up the sine layer, which will be the basic building block of SIREN. This is a much more concise implementation than the one in the main code, as here, we aren't concerned with the baseline comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-gM5h9KIvBul"
      },
      "outputs": [],
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "    \n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n",
        "    # hyperparameter.\n",
        "    \n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "    \n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "    def forward_with_intermediate(self, input): \n",
        "        # For visualization of activation distributions\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "    \n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords        \n",
        "\n",
        "    def forward_with_activations(self, coords, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        activations['input'] = x\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWDbrli4vBun"
      },
      "source": [
        "And finally, differential operators that allow us to leverage autograd to compute gradients, the laplacian, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibUenfQYvBuo"
      },
      "outputs": [],
      "source": [
        "def laplace(y, x):\n",
        "    grad = gradient(y, x)\n",
        "    return divergence(grad, x)\n",
        "\n",
        "\n",
        "def divergence(y, x):\n",
        "    div = 0.\n",
        "    for i in range(y.shape[-1]):\n",
        "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "    return div\n",
        "\n",
        "\n",
        "def gradient(y, x, grad_outputs=None):\n",
        "    if grad_outputs is None:\n",
        "        grad_outputs = torch.ones_like(y)\n",
        "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q65ZUXv7vBuo"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "For the image fitting and poisson experiments, we'll use the classic cameraman image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-sLUPEktvBup"
      },
      "outputs": [],
      "source": [
        "def get_cameraman_tensor(sidelength):\n",
        "    img = Image.fromarray(skimage.data.camera())        \n",
        "    transform = Compose([\n",
        "        Resize(sidelength),\n",
        "        ToTensor(),\n",
        "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "    ])\n",
        "    img = transform(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAboFe3vvBuq"
      },
      "source": [
        "<a id='section_1'></a>\n",
        "## Fitting an image\n",
        "\n",
        "First, let's simply fit that image!\n",
        "\n",
        "We seek to parameterize a greyscale image $f(x)$ with pixel coordinates $x$ with a SIREN $\\Phi(x)$.\n",
        "\n",
        "That is we seek the function $\\Phi$ such that:\n",
        "$\\mathcal{L}=\\int_{\\Omega} \\lVert \\Phi(\\mathbf{x}) - f(\\mathbf{x}) \\rVert\\mathrm{d}\\mathbf{x}$\n",
        " is minimized, in which $\\Omega$ is the domain of the image. \n",
        " \n",
        "We write a little datast that does nothing except calculating per-pixel coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dNeHGOs_vBur"
      },
      "outputs": [],
      "source": [
        "class ImageFitting(Dataset):\n",
        "    def __init__(self, sidelength):\n",
        "        super().__init__()\n",
        "        img = get_cameraman_tensor(sidelength)\n",
        "        self.pixels = img.permute(1, 2, 0).view(-1, 1)\n",
        "        self.coords = get_mgrid(sidelength, 2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords, self.pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Vr-DuP6wvBus"
      },
      "source": [
        "Let's instantiate the dataset and our Siren. As pixel coordinates are 2D, the siren has 2 input features, and since the image is grayscale, it has one output channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BNs9wlmvBut",
        "outputId": "96b2d4a2-562c-4703-c6dc-19dd98256dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Siren(\n",
              "  (net): Sequential(\n",
              "    (0): SineLayer(\n",
              "      (linear): Linear(in_features=2, out_features=256, bias=True)\n",
              "    )\n",
              "    (1): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (2): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (3): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cameraman = ImageFitting(256)\n",
        "dataloader = DataLoader(cameraman, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "img_siren = Siren(in_features=2, out_features=1, hidden_features=256, \n",
        "                  hidden_layers=3, outermost_linear=True)\n",
        "img_siren.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdwgatMFvBuu"
      },
      "source": [
        "We now fit Siren in a simple training loop. Within only hundreds of iterations, the image and its gradients are approximated well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xckUspLnvBuu"
      },
      "outputs": [],
      "source": [
        "total_steps = 500 # Since the whole image is our dataset, this just means 500 gradient descent steps.\n",
        "steps_til_summary = 10\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)    \n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "        img_grad = gradient(model_output, coords)\n",
        "        img_laplacian = laplace(model_output, coords)\n",
        "\n",
        "        fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
        "        axes[0].imshow(model_output.cpu().view(256,256).detach().numpy())\n",
        "        axes[1].imshow(img_grad.norm(dim=-1).cpu().view(256,256).detach().numpy())\n",
        "        axes[2].imshow(img_laplacian.cpu().view(256,256).detach().numpy())\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Video Version**"
      ],
      "metadata": {
        "id": "6UmPpGpFHhSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read Data**"
      ],
      "metadata": {
        "id": "tIFfy3ibU1vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "images_name = []\n",
        "images_path = []\n",
        "for path in glob.iglob('*.png'):\n",
        "  images_name.append(path.split('-')[0])\n",
        "  images_path.append(path)\n",
        "  \n",
        "meta_data = pd.DataFrame({'image_name': images_name, 'image_path': images_path})\n",
        "meta_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N8cg8DqDUztH",
        "outputId": "c95b4ba2-dbcd-4576-dec7-2269f9edb4b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           image_name                image_path\n",
              "0        cotton_candy       cotton_candy-48.png\n",
              "1            delivery           delivery-48.png\n",
              "2            discount           discount-48.png\n",
              "3              cooker             cooker-48.png\n",
              "4        kitchenwares       kitchenwares-48.png\n",
              "..                ...                       ...\n",
              "95      confectionery      confectionery-48.png\n",
              "96              mixer              mixer-48.png\n",
              "97  shampoo_dispenser  shampoo_dispenser-48.png\n",
              "98        shoppig_bag        shoppig_bag-48.png\n",
              "99    return_purchase    return_purchase-48.png\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11110164-41ad-4803-b5d6-504ff377fc7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cotton_candy</td>\n",
              "      <td>cotton_candy-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delivery</td>\n",
              "      <td>delivery-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discount</td>\n",
              "      <td>discount-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cooker</td>\n",
              "      <td>cooker-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kitchenwares</td>\n",
              "      <td>kitchenwares-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>confectionery</td>\n",
              "      <td>confectionery-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>mixer</td>\n",
              "      <td>mixer-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>shampoo_dispenser</td>\n",
              "      <td>shampoo_dispenser-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>shoppig_bag</td>\n",
              "      <td>shoppig_bag-48.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>return_purchase</td>\n",
              "      <td>return_purchase-48.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11110164-41ad-4803-b5d6-504ff377fc7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11110164-41ad-4803-b5d6-504ff377fc7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11110164-41ad-4803-b5d6-504ff377fc7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video(images, mgrids):\n",
        "  vid_length = len(images)\n",
        "  step = 2/(vid_length-1)\n",
        "  t_coords = np.arange(-1, 1+step, step)\n",
        "  N = mgrids[0].size()[0]\n",
        "  coords = torch.cat((mgrids[0], t_coords[0]*torch.ones(N, 1)), 1)\n",
        "  pixels = images[0]\n",
        "  for i in np.arange(1, vid_length):\n",
        "    N = mgrids[i].size()[0]\n",
        "    coord = torch.cat((mgrids[i], t_coords[i]*torch.ones(N, 1)), 1)\n",
        "    coords = torch.cat((coords, coord), 0)\n",
        "    pixels = torch.cat((pixels, images[i]), 0)\n",
        "  return pixels, coords"
      ],
      "metadata": {
        "id": "4-uT9ZR4H9R-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_tensor(sidelength, inverse=False):\n",
        "    img = Image.fromarray(skimage.data.camera())    \n",
        "    \n",
        "    transform = Compose([\n",
        "        Resize(sidelength),\n",
        "        ToTensor(),\n",
        "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "    ])\n",
        "    img = transform(img)\n",
        "    if inverse:\n",
        "      img = torch.transpose(img, 1, 2)\n",
        "    return img"
      ],
      "metadata": {
        "id": "oGdpYtbIIdPV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoFitting(Dataset):\n",
        "    def __init__(self, sidelength):\n",
        "        super().__init__()\n",
        "        img = get_cameraman_tensor(sidelength)\n",
        "        imgT = get_cameraman_tensor(sidelength, inverse=True)\n",
        "        video = [img, imgT, img]*33+[imgT]\n",
        "        pixels_l = []\n",
        "        coords_l = []\n",
        "        for img in video:\n",
        "          pixels_l.append(img.permute(1, 2, 0).reshape(-1, 1))\n",
        "          coords_l.append(get_mgrid(sidelength, 2))\n",
        "        self.pixels, self.coords = get_video(pixels_l, coords_l)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords, self.pixels"
      ],
      "metadata": {
        "id": "i3qQhFaIH3M1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cameraman = VideoFitting(256)\n",
        "dataloader = DataLoader(cameraman, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "img_siren = Siren(in_features=3, out_features=1, hidden_features=256, \n",
        "                  hidden_layers=3, outermost_linear=True)\n",
        "img_siren.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC5lb5b5H0iV",
        "outputId": "251867b2-eeeb-4cd3-f2d0-2196a4cfaefd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Siren(\n",
              "  (net): Sequential(\n",
              "    (0): SineLayer(\n",
              "      (linear): Linear(in_features=3, out_features=256, bias=True)\n",
              "    )\n",
              "    (1): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (2): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (3): SineLayer(\n",
              "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cameraman.coords.size())\n",
        "print(cameraman.pixels.size())\n",
        "print(cameraman.coords[:4])\n",
        "print(cameraman.pixels[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHkET0ILHy2l",
        "outputId": "cd07718f-e05b-4952-9147-8b2502ace05e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6553600, 3])\n",
            "torch.Size([6553600, 1])\n",
            "tensor([[-1.0000, -1.0000, -1.0000],\n",
            "        [-1.0000, -0.9922, -1.0000],\n",
            "        [-1.0000, -0.9843, -1.0000],\n",
            "        [-1.0000, -0.9765, -1.0000]])\n",
            "tensor([[0.5608],\n",
            "        [0.5686],\n",
            "        [0.5686],\n",
            "        [0.5608]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With video version\n",
        "total_steps = 500 # Since the whole image is our dataset, this just means 500 gradient descent steps.\n",
        "steps_til_summary = 10\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)    \n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "        # img_grad = gradient(model_output, coords)\n",
        "        # img_laplacian = laplace(model_output, coords)\n",
        "\n",
        "        fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
        "        axes[0].imshow(model_output.cpu().view(100, 256,256)[0,:,:].detach().numpy())\n",
        "        axes[1].imshow(model_output.cpu().view(100, 256,256)[1,:,:].detach().numpy())\n",
        "        axes[2].imshow(model_output.cpu().view(100, 256,256)[99,:,:].detach().numpy())\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "eIFYpXu2HgJE",
        "outputId": "84c9a027-1259-46c9-9ff9-376d6276cce0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-153bf3b2c58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_siren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b478c3c7c425>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# allows to take derivative w.r.t. input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b478c3c7c425>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momega_0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_with_intermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.25 GiB (GPU 0; 11.17 GiB total capacity; 6.42 GiB already allocated; 4.23 GiB free; 6.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOsvbIDJvBuv"
      },
      "source": [
        "<a id='out_of_range'></a>\n",
        "## Case study: Siren periodicity & out-of-range behavior\n",
        "\n",
        "It is known that the sum of two periodic signals is itself periodic with a period that is equal to the least common multiple of the periods of the two summands, if and only if the two periods are rational multiples of each other. If the ratio of the two periods is irrational, then their sum will *not* be periodic itself.\n",
        "\n",
        "Due to the floating-point representation in neural network libraries, this case cannot occur in practice, and all functions parameterized by Siren indeed have to be periodic.\n",
        "\n",
        "Yet, the period of the resulting function may in practice be several orders of magnitudes larger than the period of each Siren neuron!\n",
        "\n",
        "Let's test this with two sines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpXHil2xvBuv"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    coords = get_mgrid(2**10, 1) * 5 * np.pi\n",
        "    \n",
        "    sin_1 = torch.sin(coords)\n",
        "    sin_2 = torch.sin(coords * 2)\n",
        "    sum = sin_1 + sin_2\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16,2))\n",
        "    ax.plot(coords, sum)\n",
        "    ax.plot(coords, sin_1)\n",
        "    ax.plot(coords, sin_2)\n",
        "    plt.title(\"Rational multiple\")\n",
        "    plt.show()\n",
        "    \n",
        "    sin_1 = torch.sin(coords)\n",
        "    sin_2 = torch.sin(coords * np.pi)\n",
        "    sum = sin_1 + sin_2\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16,2))\n",
        "    ax.plot(coords, sum)\n",
        "    ax.plot(coords, sin_1)\n",
        "    ax.plot(coords, sin_2)\n",
        "    plt.title(\"Pseudo-irrational multiple\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmCrl2m6vBuw"
      },
      "source": [
        "Though the second plot looks periodic, closer inspection shows that the period of the blue line is indeed larger than the range we're sampling here. \n",
        "\n",
        "Let's take a look at what the Siren we just trained looks like outside its training domain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RWwVEmNZvBuw"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    out_of_range_coords = get_mgrid(1024, 2) * 50\n",
        "    model_out, _ = img_siren(out_of_range_coords.cuda())\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16,16))\n",
        "    ax.imshow(model_out.cpu().view(1024,1024).numpy())\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TGJa7-lsvBux"
      },
      "source": [
        "Though there is some self-similarity, the signal is not repeated on this range of (-50, 50)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqR2OyVxvBuy"
      },
      "source": [
        "## Fitting an audio signal\n",
        "<a id='section_2'></a>\n",
        "\n",
        "Here, we'll use Siren to parameterize an audio signal - i.e., we seek to parameterize an audio waverform $f(t)$  at time points $t$ by a SIREN $\\Phi$.\n",
        "\n",
        "That is we seek the function $\\Phi$ such that:  $\\mathcal{L}\\int_\\Omega \\lVert \\Phi(t) - f(t) \\rVert \\mathrm{d}t$  is minimized, in which  $\\Omega$  is the domain of the waveform.\n",
        "\n",
        "For the audio, we'll use the bach sonata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQB-RsGZvBuz"
      },
      "outputs": [],
      "source": [
        "import scipy.io.wavfile as wavfile\n",
        "import io\n",
        "from IPython.display import Audio\n",
        "\n",
        "if not os.path.exists('gt_bach.wav'):\n",
        "    !wget https://vsitzmann.github.io/siren/img/audio/gt_bach.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PzN1C2cjvBu0"
      },
      "source": [
        "Let's build a little dataset that computes coordinates for audio files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa5UH-0GvBu0"
      },
      "outputs": [],
      "source": [
        "class AudioFile(torch.utils.data.Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.rate, self.data = wavfile.read(filename)\n",
        "        self.data = self.data.astype(np.float32)\n",
        "        self.timepoints = get_mgrid(len(self.data), 1)\n",
        "\n",
        "    def get_num_samples(self):\n",
        "        return self.timepoints.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        amplitude = self.data\n",
        "        scale = np.max(np.abs(amplitude))\n",
        "        amplitude = (amplitude / scale)\n",
        "        amplitude = torch.Tensor(amplitude).view(-1, 1)\n",
        "        return self.timepoints, amplitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2LsA_QFGvBu0"
      },
      "source": [
        "Let's instantiate the Siren. As this audio signal has a much higer spatial frequency on the range of -1 to 1, we increase the $\\omega_0$ in the first layer of siren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghKqK6TKvBu1"
      },
      "outputs": [],
      "source": [
        "bach_audio = AudioFile('gt_bach.wav')\n",
        "\n",
        "dataloader = DataLoader(bach_audio, shuffle=True, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "# Note that we increase the frequency of the first layer to match the higher frequencies of the\n",
        "# audio signal. Equivalently, we could also increase the range of the input coordinates.\n",
        "audio_siren = Siren(in_features=1, out_features=1, hidden_features=256, \n",
        "                    hidden_layers=3, first_omega_0=3000, outermost_linear=True)\n",
        "audio_siren.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMqxk4uvBu1"
      },
      "source": [
        "Let's have a quick listen to ground truth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4qHxrONvBu1"
      },
      "outputs": [],
      "source": [
        "rate, _ = wavfile.read('gt_bach.wav')\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "Audio(ground_truth.squeeze().numpy(),rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGTeEf7vBu1"
      },
      "source": [
        "We now fit the Siren to this signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LbK7cAfvBu2"
      },
      "outputs": [],
      "source": [
        "total_steps = 1000 \n",
        "steps_til_summary = 100\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=audio_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = audio_siren(model_input)    \n",
        "    loss = F.mse_loss(model_output, ground_truth)\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "    \n",
        "        fig, axes = plt.subplots(1,2)\n",
        "        axes[0].plot(coords.squeeze().detach().cpu().numpy(),model_output.squeeze().detach().cpu().numpy())\n",
        "        axes[1].plot(coords.squeeze().detach().cpu().numpy(),ground_truth.squeeze().detach().cpu().numpy())\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XMTMwx4MvBu3"
      },
      "outputs": [],
      "source": [
        "final_model_output, coords = audio_siren(model_input)\n",
        "Audio(final_model_output.cpu().detach().squeeze().numpy(),rate=rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RsEdo12RvBu3"
      },
      "source": [
        "As we can see, within few iterations, Siren has approximated the audio signal very well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lkIlb6fvBu3"
      },
      "source": [
        "<a id='section_3'></a>\n",
        "## Solving Poisson's equation\n",
        "\n",
        "Now, let's make it a bit harder. Let's say we want to reconstruct an image but we only have access to its gradients!\n",
        "\n",
        "That is, we now seek the function $\\Phi$ such that:\n",
        "$\\mathcal{L}=\\int_{\\Omega} \\lVert \\nabla\\Phi(\\mathbf{x}) - \\nabla f(\\mathbf{x}) \\rVert\\mathrm{d}\\mathbf{x}$\n",
        " is minimized, in which $\\Omega$ is the domain of the image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cU_TpTlgvBu3"
      },
      "outputs": [],
      "source": [
        "import scipy.ndimage\n",
        "    \n",
        "class PoissonEqn(Dataset):\n",
        "    def __init__(self, sidelength):\n",
        "        super().__init__()\n",
        "        img = get_cameraman_tensor(sidelength)\n",
        "        \n",
        "        # Compute gradient and laplacian       \n",
        "        grads_x = scipy.ndimage.sobel(img.numpy(), axis=1).squeeze(0)[..., None]\n",
        "        grads_y = scipy.ndimage.sobel(img.numpy(), axis=2).squeeze(0)[..., None]\n",
        "        grads_x, grads_y = torch.from_numpy(grads_x), torch.from_numpy(grads_y)\n",
        "                \n",
        "        self.grads = torch.stack((grads_x, grads_y), dim=-1).view(-1, 2)\n",
        "        self.laplace = scipy.ndimage.laplace(img.numpy()).squeeze(0)[..., None]\n",
        "        self.laplace = torch.from_numpy(self.laplace)\n",
        "        \n",
        "        self.pixels = img.permute(1, 2, 0).view(-1, 1)\n",
        "        self.coords = get_mgrid(sidelength, 2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.coords, {'pixels':self.pixels, 'grads':self.grads, 'laplace':self.laplace}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "mdecETQJvBu4"
      },
      "source": [
        "#### Instantiate SIREN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uj7g4pkvvBu4"
      },
      "outputs": [],
      "source": [
        "cameraman_poisson = PoissonEqn(128)\n",
        "dataloader = DataLoader(cameraman_poisson, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "poisson_siren = Siren(in_features=2, out_features=1, hidden_features=256, \n",
        "                      hidden_layers=3, outermost_linear=True)\n",
        "poisson_siren.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "H0wM0Cq-vBu4"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ns5xXIYXvBu5"
      },
      "outputs": [],
      "source": [
        "def gradients_mse(model_output, coords, gt_gradients):\n",
        "    # compute gradients on the model\n",
        "    gradients = gradient(model_output, coords)\n",
        "    # compare them with the ground-truth\n",
        "    gradients_loss = torch.mean((gradients - gt_gradients).pow(2).sum(-1))\n",
        "    return gradients_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TKjLVgMyvBu5"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "_zBhvF5evBu5"
      },
      "outputs": [],
      "source": [
        "total_steps = 1000\n",
        "steps_til_summary = 10\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=poisson_siren.parameters())\n",
        "\n",
        "model_input, gt = next(iter(dataloader))\n",
        "gt = {key: value.cuda() for key, value in gt.items()}\n",
        "model_input = model_input.cuda()\n",
        "\n",
        "for step in range(total_steps):\n",
        "    start_time = time.time()\n",
        "\n",
        "    model_output, coords = poisson_siren(model_input)\n",
        "    train_loss = gradients_mse(model_output, coords, gt['grads'])\n",
        "\n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f, iteration time %0.6f\" % (step, train_loss, time.time() - start_time))\n",
        "\n",
        "        img_grad = gradient(model_output, coords)\n",
        "        img_laplacian = laplace(model_output, coords)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        axes[0].imshow(model_output.cpu().view(128,128).detach().numpy())\n",
        "        axes[1].imshow(img_grad.cpu().norm(dim=-1).view(128,128).detach().numpy())\n",
        "        axes[2].imshow(img_laplacian.cpu().view(128,128).detach().numpy())\n",
        "        plt.show()\n",
        "        \n",
        "    optim.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "M7s07r9rvBu6"
      },
      "source": [
        "<a id='activations'></a>\n",
        "## Initialization scheme & distribution of activations\n",
        "\n",
        "We now reproduce the empirical result on the distribution of activations, and will thereafter show empirically that the distribution of activations is shift-invariant as well! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G_sMES3uvBu6"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import matplotlib\n",
        "import numpy.fft as fft\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "def eformat(f, prec, exp_digits):\n",
        "    s = \"%.*e\"%(prec, f)\n",
        "    mantissa, exp = s.split('e')\n",
        "    # add 1 to digits as 1 is taken by sign +/-\n",
        "    return \"%se%+0*d\"%(mantissa, exp_digits+1, int(exp))\n",
        "\n",
        "def format_x_ticks(x, pos):\n",
        "    \"\"\"Format odd tick positions\n",
        "    \"\"\"\n",
        "    return eformat(x, 0, 1)\n",
        "\n",
        "def format_y_ticks(x, pos):\n",
        "    \"\"\"Format odd tick positions\n",
        "    \"\"\"\n",
        "    return eformat(x, 0, 1)\n",
        "\n",
        "def get_spectrum(activations):\n",
        "    n = activations.shape[0]\n",
        "\n",
        "    spectrum = fft.fft(activations.numpy().astype(np.double).sum(axis=-1), axis=0)[:n//2]\n",
        "    spectrum = np.abs(spectrum)\n",
        "\n",
        "    max_freq = 100                \n",
        "    freq = fft.fftfreq(n, 2./n)[:n//2]\n",
        "    return freq[:max_freq], spectrum[:max_freq]\n",
        "\n",
        "\n",
        "def plot_all_activations_and_grads(activations):\n",
        "    num_cols = 4\n",
        "    num_rows = len(activations)\n",
        "    \n",
        "    fig_width = 5.5\n",
        "    fig_height = num_rows/num_cols*fig_width\n",
        "    fig_height = 9\n",
        "    \n",
        "    fontsize = 5\n",
        "        \n",
        "    fig, axs = plt.subplots(num_rows, num_cols, gridspec_kw={'hspace': 0.3, 'wspace': 0.2},\n",
        "                            figsize=(fig_width, fig_height), dpi=300)\n",
        "    \n",
        "    axs[0][0].set_title(\"Activation Distribution\", fontsize=7, fontfamily='serif', pad=5.)\n",
        "    axs[0][1].set_title(\"Activation Spectrum\", fontsize=7, fontfamily='serif', pad=5.)\n",
        "    axs[0][2].set_title(\"Gradient Distribution\", fontsize=7, fontfamily='serif', pad=5.)\n",
        "    axs[0][3].set_title(\"Gradient Spectrum\", fontsize=7, fontfamily='serif', pad=5.)\n",
        "\n",
        "    x_formatter = matplotlib.ticker.FuncFormatter(format_x_ticks)\n",
        "    y_formatter = matplotlib.ticker.FuncFormatter(format_y_ticks)\n",
        "\n",
        "    spec_rows = []\n",
        "    for idx, (key, value) in enumerate(activations.items()):    \n",
        "        grad_value = value.grad.cpu().detach().squeeze(0)\n",
        "        flat_grad = grad_value.view(-1)\n",
        "        axs[idx][2].hist(flat_grad, bins=256, density=True)\n",
        "        \n",
        "        value = value.cpu().detach().squeeze(0) # (1, num_points, 256)\n",
        "        n = value.shape[0]\n",
        "        flat_value = value.view(-1)\n",
        "            \n",
        "        axs[idx][0].hist(flat_value, bins=256, density=True)\n",
        "                \n",
        "        if idx>1:\n",
        "            if not (idx)%2:\n",
        "                x = np.linspace(-1, 1., 500)\n",
        "                axs[idx][0].plot(x, stats.arcsine.pdf(x, -1, 2), \n",
        "                                 linestyle=':', markersize=0.4, zorder=2)\n",
        "            else:\n",
        "                mu = 0\n",
        "                variance = 1\n",
        "                sigma = np.sqrt(variance)\n",
        "                x = np.linspace(mu - 3*sigma, mu + 3*sigma, 500)\n",
        "                axs[idx][0].plot(x, stats.norm.pdf(x, mu, sigma), \n",
        "                                 linestyle=':', markersize=0.4, zorder=2)\n",
        "        \n",
        "        activ_freq, activ_spec = get_spectrum(value)\n",
        "        axs[idx][1].plot(activ_freq, activ_spec)\n",
        "        \n",
        "        grad_freq, grad_spec = get_spectrum(grad_value)\n",
        "        axs[idx][-1].plot(grad_freq, grad_spec)\n",
        "        \n",
        "        for ax in axs[idx]:\n",
        "            ax.tick_params(axis='both', which='major', direction='in',\n",
        "                                    labelsize=fontsize, pad=1., zorder=10) \n",
        "            ax.tick_params(axis='x', labelrotation=0, pad=1.5, zorder=10) \n",
        "\n",
        "            ax.xaxis.set_major_formatter(x_formatter)\n",
        "            ax.yaxis.set_major_formatter(y_formatter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rnWco4CTvBu6"
      },
      "outputs": [],
      "source": [
        "model = Siren(in_features=1, hidden_features=2048, \n",
        "              hidden_layers=10, out_features=1, outermost_linear=True)\n",
        "\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, 65536//4, 1)\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\n",
        "output = activations[next(reversed(activations))]\n",
        "\n",
        "# Compute gradients. Because we have retain_grad=True on \n",
        "# activations, each activation stores its own gradient!\n",
        "output.mean().backward()\n",
        "\n",
        "plot_all_activations_and_grads(activations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQqBm7YvBu7"
      },
      "source": [
        "Note how the activations of Siren always alternate between a standard normal distribution with standard deviation one, and an arcsine distribution. If you have a beefy computer, you can put this to the extreme and increase the number of layers - this property holds even for more than 50 layers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McSHttojvBu7"
      },
      "source": [
        "<a id='shift_invariance'></a>\n",
        "## Distribution of activations is shift-invariant\n",
        "\n",
        "One of the key properties of the periodic sine nonlinearity is that it affords a degree of shift-invariance. Consider the first layer of a Siren: You can convince yourself that this layer can easily learn to map two different coordinates to *the same set of activations*. This means that whatever layers come afterwards will apply the same function to these two sets of coordinates.\n",
        "\n",
        "Moreoever, the distribution of activations similarly are shift-invariant. Let's shift our input signal by 1000 and re-compute the activations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "izMirRF0vBu7"
      },
      "outputs": [],
      "source": [
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, 65536//4, 1) + 1000\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\n",
        "output = activations[next(reversed(activations))]\n",
        "\n",
        "# Compute gradients. Because we have retain_grad=True on \n",
        "# activations, each activation stores its own gradient!\n",
        "output.mean().backward()\n",
        "\n",
        "plot_all_activations_and_grads(activations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFkabnr-vBu7"
      },
      "source": [
        "As we can see, the distributions of activations didn't change at all - they are perfectly invariant to the shift."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "explore_siren.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}